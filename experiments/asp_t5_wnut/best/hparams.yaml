adam_eps: 1.0e-08
adam_weight_decay: 0.1
asp_activation: relu
asp_dropout_rate: 0.3
asp_hidden_dim: 150
asp_init_std: 0.02
batch_size: 32
beam_size: 1
fused: true
gradient_accumulation_steps: 5
is_encoder_decoder: true
max_nest_depth: 1
mention_end_token: </m>
mention_start_token: <m>
num_epochs: 80
num_labels: 6
plm_learning_rate: 5.0e-05
plm_pretrained_name_or_path: t5-base
plm_scheduler: linear_with_warmup
plm_tokenizer_name: t5-small
task_learning_rate: 0.0003
task_scheduler: linear_with_warmup
train_len: 3395
vocab_size: 32102
warmup_ratio: 0.05
