{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Build ASR pipeline\n",
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation error:  datasets/wnut/emerging.test.conll 18828 ['Advertise', 'I-creative-work']\n",
      "Annotation error:  datasets/wnut/emerging.test.conll 18829 ['Anything', 'I-creative-work']\n"
     ]
    }
   ],
   "source": [
    "from data_preparation.wnut import wnut_to_json\n",
    "\n",
    "wnut_to_json(\"datasets/wnut/wnut17train.conll\", \n",
    "                  \"datasets/wnut/emerging.dev.conll\",\n",
    "                  \"datasets/wnut/emerging.test.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': {'short': 'location'},\n",
       " 'group': {'short': 'group'},\n",
       " 'corporation': {'short': 'corporation'},\n",
       " 'person': {'short': 'person'},\n",
       " 'creative-work': {'short': 'creative-work'},\n",
       " 'product': {'short': 'product'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['For',\n",
       "  'more',\n",
       "  'info',\n",
       "  'about',\n",
       "  'this',\n",
       "  'and',\n",
       "  'local',\n",
       "  'views',\n",
       "  'on',\n",
       "  'the',\n",
       "  'matter',\n",
       "  'check',\n",
       "  'out',\n",
       "  'where',\n",
       "  'OP',\n",
       "  'took',\n",
       "  'this',\n",
       "  'from',\n",
       "  '.'],\n",
       " 'extended': ['For',\n",
       "  'more',\n",
       "  'info',\n",
       "  'about',\n",
       "  'this',\n",
       "  'and',\n",
       "  'local',\n",
       "  'views',\n",
       "  'on',\n",
       "  'the',\n",
       "  'matter',\n",
       "  'check',\n",
       "  'out',\n",
       "  'where',\n",
       "  'OP',\n",
       "  'took',\n",
       "  'this',\n",
       "  'from',\n",
       "  '.'],\n",
       " 'entities': [Entity(type='creative-work', start=6, end=8)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinloebbert/opt/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessing.tokenize import tokenize_json\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", model_max_length=4096)\n",
    "\n",
    "tokenize_json(tokenizer,\n",
    "              \"datasets/wnut/wnut17train.json\",\n",
    "              \"datasets/wnut/emerging.dev.json\",\n",
    "              \"datasets/wnut/emerging.test.json\",\n",
    "              \"datasets/wnut/wnut_types.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels can have multiple values concatenated via commas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loebbert/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessing.tensorize import NERDataProcessor\n",
    "from data_preprocessing.tokenize import MENTION_START, MENTION_END\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", model_max_length=4096)\n",
    "tokenizer.add_tokens(MENTION_START)\n",
    "tokenizer.add_tokens(MENTION_END)\n",
    "\n",
    "config = {\n",
    "    \"mention_start_token\": MENTION_START,\n",
    "    \"mention_end_token\": MENTION_END\n",
    "}\n",
    "\n",
    "processor = NERDataProcessor(config, tokenizer,\n",
    "                             \"datasets/wnut/wnut17train.t5-small.jsonlines\",\n",
    "                             \"datasets/wnut/emerging.dev.t5-small.jsonlines\",\n",
    "                             \"datasets/wnut/emerging.test.t5-small.jsonlines\",\n",
    "                             \"datasets/wnut/wnut_types.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/wnut/cached.tensors.t5-small.bin'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.get_cache_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = processor.get_tensor_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': 'emerging.test_22',\n",
       " 'sentence': ['▁Rep',\n",
       "  'ly',\n",
       "  'ing',\n",
       "  '▁to',\n",
       "  '▁another',\n",
       "  '▁question',\n",
       "  ',',\n",
       "  '▁B',\n",
       "  'ham',\n",
       "  're',\n",
       "  '▁said',\n",
       "  '▁the',\n",
       "  '▁jaw',\n",
       "  'ans',\n",
       "  '▁deployed',\n",
       "  '▁at',\n",
       "  '▁places',\n",
       "  '▁such',\n",
       "  '▁as',\n",
       "  '▁Si',\n",
       "  'a',\n",
       "  'chen',\n",
       "  '▁Gla',\n",
       "  'cier',\n",
       "  '▁are',\n",
       "  '▁provided',\n",
       "  '▁with',\n",
       "  '▁the',\n",
       "  '▁best',\n",
       "  '-',\n",
       "  'quality',\n",
       "  '▁winter',\n",
       "  '▁clothing',\n",
       "  '.',\n",
       "  '</s>'],\n",
       " 'input_sentence': ['▁named',\n",
       "  '▁entity',\n",
       "  '▁recognition',\n",
       "  ':',\n",
       "  '▁Rep',\n",
       "  'ly',\n",
       "  'ing',\n",
       "  '▁to',\n",
       "  '▁another',\n",
       "  '▁question',\n",
       "  ',',\n",
       "  '▁B',\n",
       "  'ham',\n",
       "  're',\n",
       "  '▁said',\n",
       "  '▁the',\n",
       "  '▁jaw',\n",
       "  'ans',\n",
       "  '▁deployed',\n",
       "  '▁at',\n",
       "  '▁places',\n",
       "  '▁such',\n",
       "  '▁as',\n",
       "  '▁Si',\n",
       "  'a',\n",
       "  'chen',\n",
       "  '▁Gla',\n",
       "  'cier',\n",
       "  '▁are',\n",
       "  '▁provided',\n",
       "  '▁with',\n",
       "  '▁the',\n",
       "  '▁best',\n",
       "  '-',\n",
       "  'quality',\n",
       "  '▁winter',\n",
       "  '▁clothing',\n",
       "  '.',\n",
       "  '</s>'],\n",
       " 'target_sentence': ['▁Rep',\n",
       "  'ly',\n",
       "  'ing',\n",
       "  '▁to',\n",
       "  '▁another',\n",
       "  '▁question',\n",
       "  ',',\n",
       "  '<m>',\n",
       "  '<m>',\n",
       "  '▁B',\n",
       "  'ham',\n",
       "  're',\n",
       "  '</m>',\n",
       "  '</m>',\n",
       "  '▁said',\n",
       "  '▁the',\n",
       "  '<m>',\n",
       "  '▁jaw',\n",
       "  'ans',\n",
       "  '</m>',\n",
       "  '▁deployed',\n",
       "  '▁at',\n",
       "  '▁places',\n",
       "  '▁such',\n",
       "  '▁as',\n",
       "  '<m>',\n",
       "  '<m>',\n",
       "  '<m>',\n",
       "  '▁Si',\n",
       "  'a',\n",
       "  'chen',\n",
       "  '▁Gla',\n",
       "  'cier',\n",
       "  '</m>',\n",
       "  '</m>',\n",
       "  '</m>',\n",
       "  '▁are',\n",
       "  '▁provided',\n",
       "  '▁with',\n",
       "  '▁the',\n",
       "  '▁best',\n",
       "  '-',\n",
       "  'quality',\n",
       "  '▁winter',\n",
       "  '▁clothing',\n",
       "  '.',\n",
       "  '</s>'],\n",
       " 'subtoken_map': [0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24],\n",
       " 'ent_type_sequence': [-1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  3,\n",
       "  4,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1],\n",
       " 'ent_indices': [-1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  4,\n",
       "  5,\n",
       "  3,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  4,\n",
       "  5,\n",
       "  3,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.stored_info[\"example\"][\"emerging.test_22\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "test_multiple = None\n",
    "for (doc_key, subtoken_map, sample) in test.data:\n",
    "    if doc_key == \"emerging.test_22\":\n",
    "        test_multiple = (doc_key, subtoken_map, sample)\n",
    "        break\n",
    "with open(\"tests/data/wnut_nested_batch_1.pkl\", \"wb\") as file:\n",
    "    pickle.dump(test_multiple, file)\n",
    "\n",
    "with open(\"tests/data/wnut_nested_batch_10.pkl\", \"wb\") as file:\n",
    "    res = [test_multiple, *test.data[:9]]\n",
    "    print(len(res))\n",
    "    pickle.dump(res, file)\n",
    "\n",
    "with open(\"tests/data/wnut_batch_1.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train.data[0], file)\n",
    "\n",
    "with open(\"tests/data/wnut_batch_10.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train.data[:10], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6666666666666667, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5/3, 5//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing.tensorize import ner_collate_fn\n",
    "import pickle\n",
    "\n",
    "with open(\"tests/data/wnut_batch_10.pkl\", \"rb\") as file:\n",
    "            data_point = pickle.load(file)\n",
    "(doc_keys, subtoken_maps, batch) = ner_collate_fn(data_point)\n",
    "assert len(subtoken_maps) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  18,\n",
       "  18,\n",
       "  18,\n",
       "  18,\n",
       "  19,\n",
       "  19,\n",
       "  19,\n",
       "  19,\n",
       "  19,\n",
       "  19,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30],\n",
       " [0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  24,\n",
       "  25,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  31,\n",
       "  32,\n",
       "  32,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  34,\n",
       "  34,\n",
       "  34,\n",
       "  34,\n",
       "  34,\n",
       "  35],\n",
       " [0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  21,\n",
       "  21,\n",
       "  21,\n",
       "  22,\n",
       "  22,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  27,\n",
       "  27,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  29,\n",
       "  29,\n",
       "  29,\n",
       "  29,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  31],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  18,\n",
       "  19,\n",
       "  19,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  27,\n",
       "  27,\n",
       "  27,\n",
       "  27,\n",
       "  28],\n",
       " [0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  18,\n",
       "  18,\n",
       "  18,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  21,\n",
       "  21,\n",
       "  21,\n",
       "  21,\n",
       "  21,\n",
       "  21,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  25],\n",
       " [0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  19,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  24,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  26,\n",
       "  27],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  18,\n",
       "  19,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  21,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  25,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  29,\n",
       "  30,\n",
       "  30,\n",
       "  31,\n",
       "  31,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  34,\n",
       "  35],\n",
       " [0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  24,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  40,\n",
       "  41],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  18,\n",
       "  18,\n",
       "  19,\n",
       "  19,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  27,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  29],\n",
       " [0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  21,\n",
       "  21,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  24,\n",
       "  25,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  29,\n",
       "  29,\n",
       "  30])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtoken_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinloebbert/opt/miniconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5Model.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "\n",
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/loebbert/projects'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\"/\" + os.path.join(*os.getcwd().split(os.path.sep)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loebbert/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models.metrics import F1ASP\n",
    "\n",
    "metric = F1ASP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.update([(2, 3, 5)], [])\n",
    "metric.update([], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6530612707138062"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
