{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Build ASR pipeline\n",
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation error:  datasets/wnut/emerging.test.conll 18828 ['Advertise', 'I-creative-work']\n",
      "Annotation error:  datasets/wnut/emerging.test.conll 18829 ['Anything', 'I-creative-work']\n"
     ]
    }
   ],
   "source": [
    "from data_preparation.wnut import wnut_to_json\n",
    "\n",
    "wnut_to_json(\"datasets/wnut/wnut17train.conll\", \n",
    "                  \"datasets/wnut/emerging.dev.conll\",\n",
    "                  \"datasets/wnut/emerging.test.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': {'short': 'location'},\n",
       " 'group': {'short': 'group'},\n",
       " 'corporation': {'short': 'corporation'},\n",
       " 'person': {'short': 'person'},\n",
       " 'creative-work': {'short': 'creative-work'},\n",
       " 'product': {'short': 'product'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['For',\n",
       "  'more',\n",
       "  'info',\n",
       "  'about',\n",
       "  'this',\n",
       "  'and',\n",
       "  'local',\n",
       "  'views',\n",
       "  'on',\n",
       "  'the',\n",
       "  'matter',\n",
       "  'check',\n",
       "  'out',\n",
       "  'where',\n",
       "  'OP',\n",
       "  'took',\n",
       "  'this',\n",
       "  'from',\n",
       "  '.'],\n",
       " 'extended': ['For',\n",
       "  'more',\n",
       "  'info',\n",
       "  'about',\n",
       "  'this',\n",
       "  'and',\n",
       "  'local',\n",
       "  'views',\n",
       "  'on',\n",
       "  'the',\n",
       "  'matter',\n",
       "  'check',\n",
       "  'out',\n",
       "  'where',\n",
       "  'OP',\n",
       "  'took',\n",
       "  'this',\n",
       "  'from',\n",
       "  '.'],\n",
       " 'entities': [Entity(type='creative-work', start=6, end=8)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinloebbert/opt/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessing.tokenize import tokenize_json\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", model_max_length=4096)\n",
    "\n",
    "tokenize_json(tokenizer,\n",
    "              \"datasets/wnut/wnut17train.json\",\n",
    "              \"datasets/wnut/emerging.dev.json\",\n",
    "              \"datasets/wnut/emerging.test.json\",\n",
    "              \"datasets/wnut/wnut_types.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels can have multiple values concatenated via commas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing.tensorize import NERDataProcessor\n",
    "from data_preprocessing.tokenize import MENTION_START, MENTION_END\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", model_max_length=4096)\n",
    "tokenizer.add_tokens(MENTION_START)\n",
    "tokenizer.add_tokens(MENTION_END)\n",
    "\n",
    "processor = NERDataProcessor({}, tokenizer, \n",
    "                             MENTION_START, MENTION_END,\n",
    "                             \"datasets/wnut/wnut17train.t5-small.jsonlines\",\n",
    "                             \"datasets/wnut/emerging.dev.t5-small.jsonlines\",\n",
    "                             \"datasets/wnut/emerging.test.t5-small.jsonlines\",\n",
    "                             \"datasets/wnut/wnut_types.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = processor.get_tensor_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "test_multiple = None\n",
    "for (key, value) in test.data:\n",
    "    if key == \"emerging.test_22\":\n",
    "        test_multiple = (key, value)\n",
    "        break\n",
    "with open(\"tests/data/wnut_nested_batch_1.pkl\", \"wb\") as file:\n",
    "    pickle.dump(test_multiple, file)\n",
    "\n",
    "with open(\"tests/data/wnut_nested_batch_10.pkl\", \"wb\") as file:\n",
    "    res = [test_multiple, *test.data[:9]]\n",
    "    print(len(res))\n",
    "    pickle.dump(res, file)\n",
    "\n",
    "with open(\"tests/data/wnut_batch_1.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train.data[0], file)\n",
    "\n",
    "with open(\"tests/data/wnut_batch_10.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train.data[:10], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6666666666666667, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5/3, 5//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing.tensorize import ner_collate_fn\n",
    "import pickle\n",
    "\n",
    "with open(\"tests/data/wnut_batch_1.pkl\", \"rb\") as file:\n",
    "            data_point = pickle.load(file)\n",
    "batch = ner_collate_fn([data_point])\n",
    "assert len(batch) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('wnut17train_395', 'wnut17train_395'),\n",
       " {'input_ids': tensor([[ 2650, 10409,  5786,    10,     3,  5934,  3320,   439,  8399,    23,\n",
       "            1191,   107,   254,     3,  5934,  3320,   134,  2295,     9, 13286,\n",
       "           13714,    17,   109,    51,    29,  3337,   116,  3320,     7,   994,\n",
       "              77,    77,    23,    19,     3,  5307,     3,    40,    17,   140,\n",
       "             474,     8,   505,  2423,  2423,  2423,  2423,  3274,   308,    30,\n",
       "             160,    58,     3,   184,    40,    17,   117,     3,    40,    51,\n",
       "              89,     9,    32,     3, 14952, 16780,  2381,    41,     3,    29,\n",
       "              77,    23,     3,   157,    29,   210,     7,     3,   210,     7,\n",
       "             413,  4244,     3,    76,   416, 16497,    61,     1],\n",
       "          [ 2650, 10409,  5786,    10,     3,  5934,  3320,   439,  8399,    23,\n",
       "            1191,   107,   254,     3,  5934,  3320,   134,  2295,     9, 13286,\n",
       "           13714,    17,   109,    51,    29,  3337,   116,  3320,     7,   994,\n",
       "              77,    77,    23,    19,     3,  5307,     3,    40,    17,   140,\n",
       "             474,     8,   505,  2423,  2423,  2423,  2423,  3274,   308,    30,\n",
       "             160,    58,     3,   184,    40,    17,   117,     3,    40,    51,\n",
       "              89,     9,    32,     3, 14952, 16780,  2381,    41,     3,    29,\n",
       "              77,    23,     3,   157,    29,   210,     7,     3,   210,     7,\n",
       "             413,  4244,     3,    76,   416, 16497,    61,     1]]),\n",
       "  'input_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       "  'to_copy_ids': tensor([[    3,  5934,  3320,   439,  8399,    23,  1191,   107,   254,     3,\n",
       "            5934,  3320,   134,  2295,     9, 13286, 13714,    17,   109,    51,\n",
       "              29,  3337,   116,  3320,     7,   994,    77,    77,    23,    19,\n",
       "               3,  5307,     3,    40,    17,   140,   474,     8,   505,  2423,\n",
       "            2423,  2423,  2423,  3274,   308,    30,   160,    58,     3,   184,\n",
       "              40,    17,   117,     3,    40,    51,    89,     9,    32,     3,\n",
       "           14952, 16780,  2381,    41,     3,    29,    77,    23,     3,   157,\n",
       "              29,   210,     7,     3,   210,     7,   413,  4244,     3,    76,\n",
       "             416, 16497,    61,     1],\n",
       "          [    3,  5934,  3320,   439,  8399,    23,  1191,   107,   254,     3,\n",
       "            5934,  3320,   134,  2295,     9, 13286, 13714,    17,   109,    51,\n",
       "              29,  3337,   116,  3320,     7,   994,    77,    77,    23,    19,\n",
       "               3,  5307,     3,    40,    17,   140,   474,     8,   505,  2423,\n",
       "            2423,  2423,  2423,  3274,   308,    30,   160,    58,     3,   184,\n",
       "              40,    17,   117,     3,    40,    51,    89,     9,    32,     3,\n",
       "           14952, 16780,  2381,    41,     3,    29,    77,    23,     3,   157,\n",
       "              29,   210,     7,     3,   210,     7,   413,  4244,     3,    76,\n",
       "             416, 16497,    61,     1]]),\n",
       "  'target_ids': tensor([[    3,  5934,  3320,   439,  8399,    23,  1191,   107,   254,     3,\n",
       "            5934,  3320,   134,  2295,     9, 13286, 13714,    17,   109,    51,\n",
       "              29,  3337,   116,  3320,     7,   994,    77,    77,    23,    19,\n",
       "               3,  5307,     3,    40,    17,   140,   474,     8,   505,  2423,\n",
       "            2423,  2423,  2423,  3274,   308,    30,   160,    58,     3,   184,\n",
       "              40,    17,   117,     3,    40,    51,    89,     9,    32,     3,\n",
       "           14952, 16780,  2381,    41, 32100,     3,    29,    77,    23, 32101,\n",
       "               3,   157,    29,   210,     7,     3,   210,     7,   413,  4244,\n",
       "               3,    76,   416, 16497,    61,     1],\n",
       "          [    3,  5934,  3320,   439,  8399,    23,  1191,   107,   254,     3,\n",
       "            5934,  3320,   134,  2295,     9, 13286, 13714,    17,   109,    51,\n",
       "              29,  3337,   116,  3320,     7,   994,    77,    77,    23,    19,\n",
       "               3,  5307,     3,    40,    17,   140,   474,     8,   505,  2423,\n",
       "            2423,  2423,  2423,  3274,   308,    30,   160,    58,     3,   184,\n",
       "              40,    17,   117,     3,    40,    51,    89,     9,    32,     3,\n",
       "           14952, 16780,  2381,    41, 32100,     3,    29,    77,    23, 32101,\n",
       "               3,   157,    29,   210,     7,     3,   210,     7,   413,  4244,\n",
       "               3,    76,   416, 16497,    61,     1]]),\n",
       "  'target_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       "  'action_labels': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ent_indices': tensor([[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "          [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]),\n",
       "  'ent_types': tensor([[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  3, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "          [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  3, -1, -1,\n",
       "           -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]),\n",
       "  'lr_pair_flag': tensor([[[[False, False, False, False, False, False]],\n",
       "  \n",
       "           [[False, False, False, False, False, False]],\n",
       "  \n",
       "           [[False, False, False, False, False, False]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[False, False, False, False, False, False]],\n",
       "  \n",
       "           [[False, False, False, False, False, False]],\n",
       "  \n",
       "           [[False, False, False, False, False, False]]],\n",
       "  \n",
       "  \n",
       "          [[[False, False, False, False, False, False]],\n",
       "  \n",
       "           [[False, False, False, False, False, False]],\n",
       "  \n",
       "           [[False, False, False, False, False, False]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[False, False, False, False, False, False]],\n",
       "  \n",
       "           [[False, False, False, False, False, False]],\n",
       "  \n",
       "           [[False, False, False, False, False, False]]]]),\n",
       "  'is_training': tensor([True, True])})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinloebbert/opt/miniconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5Model.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "\n",
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/loebbert/projects'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\"/\" + os.path.join(*os.getcwd().split(os.path.sep)[:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
